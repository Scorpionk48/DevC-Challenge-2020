# Week 7
## Tree-based model, Bagging & Boosting
### Objectives
Bagging is an ensemble learning technique. The idea is training individual classifiers with bootstrap samples (uniformly randomly sample with replacement) and then make predictions based on majority voting.  
In boosting, the ensemble also consists of weak learners (e.g. Decision Tree). The key concept behind boosting is to focus on training samples that are hard to classify. In other words, let the weak learners subsequently learn from misclassified training samples to improve the performance of the ensemble.  
After the lecture, students will be able to understand the basic concept of:  
- Decision Tree & Random Forest
- Ensemble Learning: Bagging & Boosting
- Adaboost & XGBoost (Extreme Gradient Boosting)

### Notebooks
The material of this week is uploaded in this [Google Drive Folder](https://drive.google.com/drive/folders/1TwdPzxckTzkrVEsNHbPQ837kj6gub159)

### Additional Resources
- [Understanding Random Forest](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)
- [Simple guide for ensemble learning methods](https://towardsdatascience.com/simple-guide-for-ensemble-learning-methods-d87cc68705a2)
- [A short introduction to Boosting](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.93.5148&rep=rep1&type=pdf)
