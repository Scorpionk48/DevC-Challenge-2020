# Week 6
## KNN & SVM
### Objectives
KNN is a non-parametric classifier algorithms. It simply looks at the K points in the training set that are nearest to the input x, and assign the most common label to the output. This method is also called instance-based learning.  

Support Vector Machine (SVM) algorithm is a popular machine learning tool that offers solutions for both classification and regression problems. Developed at AT&T Bell Laboratories by Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Vapnik et al., 1997), it presents one of the most robust prediction methods, based on the statistical learning framework proposed by Vapnik and Chervonekis (1974) and Vapnik (1982, 1995).  

After the lecture, students will be able to understand:
- Bias-Variance tradeoff
- L1 & L2 Regularization
- K-Nearest Neighbors
- Curse of high dimemsionality
- K-Fold Cross Validation (K-Fold CV)
- Max margin classifier: Hard margin & soft margin
- The kernel method
- Linear SVM & Kernel SVM

### Notebooks  
The material of this week is uploaded in this [Google Drive Folder](https://drive.google.com/drive/folders/1NdLF7T1HAeExFBO9fU4alxZFWAcPQzXJ)

### Additional Resources
- [Scott Fortmann - Understanding the Bias-Variance Tradeoff](http://scott.fortmann-roe.com/docs/BiasVariance.html)
- [GeeksforGeeks - Classifying data using SVM](https://www.geeksforgeeks.org/classifying-data-using-support-vector-machinessvms-in-python/)
- [ML Mastery-SVM](https://machinelearningmastery.com/support-vector-machines-for-machine-learning/)
- [MIT - An Idiot's guide to SVM](https://web.mit.edu/6.034/wwwbob/svm.pdf)
- [The kernel cookbook](http://www.cs.toronto.edu/~duvenaud/cookbook/index.html)
